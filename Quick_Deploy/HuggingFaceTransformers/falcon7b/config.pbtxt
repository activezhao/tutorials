# Triton backend to use
backend: "python"

# Hugging face model path
# Parameters must follow this
# key/value structure
parameters: {
	key: "huggingface_model",
	value: {string_value: "tiiuae/falcon-7b"}
}

# The maximum number of tokens
# to generate in response to
# our input
parameters: {
	key: "max_output_length",
	value: {string_value: "15"}
}

# Triton should expect a single
# string of set length named prompt
# as input
input [
  {
    name: "prompt"
    data_type: TYPE_STRING
    dims: [ 1 ]
  }
]

# Triton should expect to respond
# with a single string output of
# variable length
output [
  {
    name: "text"
    data_type: TYPE_STRING
    dims: [ -1 ]
  }
]
